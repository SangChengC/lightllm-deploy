# Default values for lightllm-chart.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: gai-registry-cn-beijing.cr.volces.com/devsft-ccr-0/lightllm-server
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "latest-9bf04aeb"

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

podAnnotations: {}

podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 18001

ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths:
        - path: /
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local
resources:
  limits:
    cpu: 4000m                        # CPU 限制，2 核
    memory: 8Gi                       # 内存限制，4 GiB
    nvidia.com/gpu: 8                 # GPU 限制，1 个 GPU
  requests:
    cpu: 2000m                        # CPU 请求，1 核
    memory: 4Gi                       # 内存请求，2 GiB
    nvidia.com/gpu: 8                 # GPU 请求，1 个 GPU

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 100
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

nodeSelector: 
  sensechat.com/node-gpu: "True"

tolerations:
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"

# affinity:
#   nodeAffinity:
#     requiredDuringSchedulingIgnoredDuringExecution:
#       nodeSelectorTerms:
#         - matchExpressions:
#             - key: "accelerator"
#               operator: In
#               values:
#                 - "nvidia"

# LightLLM 启动参数
lightllm:
  startupArgs:
    model_dir: "/path/to/model/data"               # 模型目录
    host: "0.0.0.0"                        # 服务监听的主机地址
    port: 18001                            # 服务监听的端口
    nccl_port: 28769                       # NCCL 通信端口
    tp: 1                                  # Tensor 并行度
    max_req_input_len: 16384               # 请求的最大输入长度
    max_req_total_len: 16385               # 请求的最大总长度
    max_total_token_num: 16385             # 最大 token 数
    tokenizer_mode: "auto"                 # 分词器模式
    use_dynamic_prompt_cache: true         # 是否使用动态提示缓存
    trust_remote_code: true                # 是否信任远程代码
    mode: "triton_gqa_flashdecoding"       # 运行模式
    data_type: "bf16"                      # 数据类型
  additionalArgs: ""                       # 其他启动参数（可选）

 # 模型数据卷配置（如果需要挂载模型文件）
modelData:
  storageClassName: "open-local-lvm-xfs"        # StorageClass 名称
  accessModes:
    - ReadOnlyMany                              # 访问模式，根据需求调整
  size: 400Gi                                   # 请求的存储大小
  mountPath: "/path/to/model/data"              # 在容器内的挂载路径
